{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install matplotlib ipywidgets\n",
    "!pip install boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import botocore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path():\n",
    "    \"\"\"\n",
    "    Get directory path from user input.\n",
    "    \n",
    "    Returns:\n",
    "    - directory (str): The directory path entered by the user.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        directory = input(\"Enter directory path (local directory or S3 bucket prefix): \").strip()\n",
    "        if os.path.isdir(directory):\n",
    "            return directory, 'local'\n",
    "        elif directory.startswith('s3://'):\n",
    "            return directory[5:], 's3'\n",
    "        else:\n",
    "            print(\"Invalid directory path. Please enter a valid local directory or S3 bucket prefix.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Dicom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def verify_dicom_files(directory, location='local', output_csv=None):\n",
    "    \"\"\"\n",
    "    Verify if DICOM files in a directory (local or S3 bucket) and its subdirectories are valid and can be read.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    - output_csv (str): Optional. If provided, output the invalid DICOM files to a CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - valid_files (list): List of paths to valid DICOM files.\n",
    "    - invalid_files (list): List of tuples containing paths to invalid DICOM files and encountered errors.\n",
    "    \"\"\"\n",
    "    valid_files = []\n",
    "    invalid_files = []\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(root, filename)\n",
    "                if filename.endswith('.dcm'):\n",
    "                    try:\n",
    "                        pydicom.dcmread(file_path)\n",
    "                        valid_files.append(file_path)\n",
    "                    except Exception as e:\n",
    "                        invalid_files.append((file_path, str(e)))\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        # Read DICOM file directly from S3\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "                        valid_files.append(key)\n",
    "                    except Exception as e:\n",
    "                        invalid_files.append((key, str(e)))\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    if output_csv:\n",
    "        # Save invalid DICOM files to a CSV file\n",
    "        df = pd.DataFrame(invalid_files, columns=['File', 'Error'])\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        print(f\"Invalid DICOM files saved to: {output_csv}\")\n",
    "    \n",
    "    return valid_files, invalid_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "valid_files, invalid_files = verify_dicom_files(directory, location, output_csv=r'Your Path')\n",
    "print(\"Valid DICOM files:\")\n",
    "print(valid_files)\n",
    "print(\"\\nInvalid DICOM files:\")\n",
    "print(invalid_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Count Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dicom_files(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Count the number of DICOM files in a directory and its subdirectories (local or S3 bucket).\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - total_count (int): Total number of DICOM files found.\n",
    "    - counts_by_subdirectory (dict): Dictionary containing counts of DICOM files in each subdirectory.\n",
    "    \"\"\"\n",
    "    total_count = 0\n",
    "    counts_by_subdirectory = {}\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            dicom_files = [file for file in files if file.endswith('.dcm')]\n",
    "            if dicom_files:\n",
    "                counts_by_subdirectory[root] = len(dicom_files)\n",
    "                total_count += len(dicom_files)\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Count DICOM files directly from S3\n",
    "                    total_count += 1\n",
    "                    subdirectory = os.path.dirname(key[len(prefix)+1:])  # Get relative subdirectory\n",
    "                    counts_by_subdirectory.setdefault(subdirectory, 0)\n",
    "                    counts_by_subdirectory[subdirectory] += 1\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    return total_count, counts_by_subdirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "total_count, counts_by_subdirectory = count_dicom_files(directory, location)\n",
    "print(\"Total number of DICOM files:\", total_count)\n",
    "print(\"Counts by subdirectory:\", counts_by_subdirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for duplicate SOP's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate_sop_uids(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Check for duplicate SOP Instance UIDs (SOP UID) in DICOM files within a directory and its subdirectories (local or S3 bucket).\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - duplicate_uids (dict): Dictionary containing lists of files with duplicate SOP UID for each UID.\n",
    "    \"\"\"\n",
    "    duplicate_uids = defaultdict(list)\n",
    "    sop_uids = defaultdict(list)\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        ds = pydicom.dcmread(file_path)\n",
    "                        sop_uid = ds.SOPInstanceUID\n",
    "                        sop_uids[sop_uid].append(file_path)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading DICOM file '{file_path}': {e}\")\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Read DICOM file directly from S3\n",
    "                    try:\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "                        sop_uid = ds.SOPInstanceUID\n",
    "                        sop_uids[sop_uid].append(key)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading DICOM file '{key}' from S3: {e}\")\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    # Find duplicate SOP UIDs\n",
    "    for uid, files in sop_uids.items():\n",
    "        if len(files) > 1:\n",
    "            duplicate_uids[uid] = files\n",
    "    \n",
    "    return duplicate_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "duplicate_uids = check_duplicate_sop_uids(directory, location)\n",
    "print(\"Duplicate SOP Instance UIDs:\")\n",
    "for uid, files in duplicate_uids.items():\n",
    "    print(f\"SOP UID: {uid}, Files: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Dicom consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dicom_consistency(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Perform a consistency check on all DICOM files within a directory and its subdirectories (local or S3 bucket).\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - consistency_report (dict): Dictionary containing consistency check results.\n",
    "    \"\"\"\n",
    "    consistency_report = defaultdict(dict)\n",
    "    patient_ids = set()\n",
    "    study_instance_uids = set()\n",
    "    series_instance_uids = defaultdict(set)\n",
    "    sop_instance_uids = defaultdict(set)\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        ds = pydicom.dcmread(file_path)\n",
    "                        \n",
    "                        # Check presence of required attributes\n",
    "                        if 'PatientID' not in ds:\n",
    "                            consistency_report[file_path]['missing_PatientID'] = True\n",
    "                        if 'StudyInstanceUID' not in ds:\n",
    "                            consistency_report[file_path]['missing_StudyInstanceUID'] = True\n",
    "                        if 'SeriesInstanceUID' not in ds:\n",
    "                            consistency_report[file_path]['missing_SeriesInstanceUID'] = True\n",
    "                        if 'SOPInstanceUID' not in ds:\n",
    "                            consistency_report[file_path]['missing_SOPInstanceUID'] = True\n",
    "                        \n",
    "                        # Check consistency of PatientID\n",
    "                        patient_id = ds.get('PatientID', '')\n",
    "                        patient_ids.add(patient_id)\n",
    "                        \n",
    "                        # Check consistency of StudyInstanceUID\n",
    "                        study_uid = ds.get('StudyInstanceUID', '')\n",
    "                        study_instance_uids.add(study_uid)\n",
    "                        \n",
    "                        # Check consistency of SeriesInstanceUID within each study\n",
    "                        series_uid = ds.get('SeriesInstanceUID', '')\n",
    "                        series_instance_uids[study_uid].add(series_uid)\n",
    "                        \n",
    "                        # Check consistency of SOPInstanceUID within each series\n",
    "                        sop_uid = ds.get('SOPInstanceUID', '')\n",
    "                        sop_instance_uids[series_uid].add(sop_uid)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        consistency_report[file_path]['error'] = str(e)\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Read DICOM file directly from S3\n",
    "                    try:\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "                        \n",
    "                        # Check presence of required attributes\n",
    "                        if 'PatientID' not in ds:\n",
    "                            consistency_report[key]['missing_PatientID'] = True\n",
    "                        if 'StudyInstanceUID' not in ds:\n",
    "                            consistency_report[key]['missing_StudyInstanceUID'] = True\n",
    "                        if 'SeriesInstanceUID' not in ds:\n",
    "                            consistency_report[key]['missing_SeriesInstanceUID'] = True\n",
    "                        if 'SOPInstanceUID' not in ds:\n",
    "                            consistency_report[key]['missing_SOPInstanceUID'] = True\n",
    "                        \n",
    "                        # Check consistency of PatientID\n",
    "                        patient_id = ds.get('PatientID', '')\n",
    "                        patient_ids.add(patient_id)\n",
    "                        \n",
    "                        # Check consistency of StudyInstanceUID\n",
    "                        study_uid = ds.get('StudyInstanceUID', '')\n",
    "                        study_instance_uids.add(study_uid)\n",
    "                        \n",
    "                        # Check consistency of SeriesInstanceUID within each study\n",
    "                        series_uid = ds.get('SeriesInstanceUID', '')\n",
    "                        series_instance_uids[study_uid].add(series_uid)\n",
    "                        \n",
    "                        # Check consistency of SOPInstanceUID within each series\n",
    "                        sop_uid = ds.get('SOPInstanceUID', '')\n",
    "                        sop_instance_uids[series_uid].add(sop_uid)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        consistency_report[key]['error'] = str(e)\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    # Add consistency check results to the report\n",
    "    consistency_report['PatientID_consistent'] = len(patient_ids) == 1\n",
    "    consistency_report['StudyInstanceUID_consistent'] = len(study_instance_uids) == 1\n",
    "    \n",
    "    for study_uid, series_uids in series_instance_uids.items():\n",
    "        consistency_report[f'SeriesInstanceUID_consistent_{study_uid}'] = len(series_uids) == 1\n",
    "    \n",
    "    for series_uid, sop_uids in sop_instance_uids.items():\n",
    "        consistency_report[f'SOPInstanceUID_consistent_{series_uid}'] = len(sop_uids) == len(files)\n",
    "    \n",
    "    return consistency_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_dicom_files_directory()\n",
    "consistency_report = check_dicom_consistency(directory, location)\n",
    "print(\"Consistency Report:\")\n",
    "for file_path, report in consistency_report.items():\n",
    "    print(f\"File: {file_path}, Report: {report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify DICOM IOD (Dciodvfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dicom_iod_data(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Verify DICOM IOD (Information Object Definition) data consistency within DICOM files\n",
    "    in a directory and its subdirectories.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - iod_verification_report (dict): Dictionary containing IOD verification results.\n",
    "    \"\"\"\n",
    "    iod_verification_report = defaultdict(list)\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        ds = pydicom.dcmread(file_path)\n",
    "                        iod = ds.SOPClassUID\n",
    "                        \n",
    "                        if iod:\n",
    "                            iod_verification_report[iod].append(file_path)\n",
    "                        else:\n",
    "                            iod_verification_report['No SOPClassUID'].append(file_path)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        iod_verification_report['Error reading file'].append((file_path, str(e)))\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Read DICOM file directly from S3\n",
    "                    try:\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "                        iod = ds.SOPClassUID\n",
    "                        \n",
    "                        if iod:\n",
    "                            iod_verification_report[iod].append(key)\n",
    "                        else:\n",
    "                            iod_verification_report['No SOPClassUID'].append(key)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        iod_verification_report['Error reading file'].append((key, str(e)))\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    return iod_verification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "iod_verification_report = verify_dicom_iod_data(directory, location)\n",
    "print(\"IOD Verification Report:\")\n",
    "for iod, files in iod_verification_report.items():\n",
    "    print(f\"IOD: {iod}, Files: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate activity report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_activity_report(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Generate an activity report for DICOM files within a directory and its subdirectories.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory to search for DICOM files (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - activity_report (dict): Dictionary containing the activity report.\n",
    "    \"\"\"\n",
    "    activity_report = defaultdict(int)\n",
    "    patients = set()\n",
    "    studies = set()\n",
    "    series = set()\n",
    "    modalities = set()\n",
    "    \n",
    "    if location == 'local':\n",
    "        # Traverse through all subdirectories recursively in the local directory\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        ds = pydicom.dcmread(file_path)\n",
    "                        \n",
    "                        # Count patients, studies, series, modalities\n",
    "                        patients.add(ds.PatientID)\n",
    "                        studies.add(ds.StudyInstanceUID)\n",
    "                        series.add(ds.SeriesInstanceUID)\n",
    "                        modalities.add(ds.Modality)\n",
    "                        \n",
    "                        # Count total DICOM files\n",
    "                        activity_report['Total DICOM Files'] += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        activity_report['Error reading file'] += 1\n",
    "    elif location == 's3':\n",
    "        # Traverse through all objects in the S3 bucket prefix\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Read DICOM file directly from S3\n",
    "                    try:\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "                        \n",
    "                        # Count patients, studies, series, modalities\n",
    "                        patients.add(ds.PatientID)\n",
    "                        studies.add(ds.StudyInstanceUID)\n",
    "                        series.add(ds.SeriesInstanceUID)\n",
    "                        modalities.add(ds.Modality)\n",
    "                        \n",
    "                        # Count total DICOM files\n",
    "                        activity_report['Total DICOM Files'] += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        activity_report['Error reading file'] += 1\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "    \n",
    "    # Update activity report with counts\n",
    "    activity_report['Total Patients'] = len(patients)\n",
    "    activity_report['Total Studies'] = len(studies)\n",
    "    activity_report['Total Series'] = len(series)\n",
    "    activity_report['Total Modalities'] = len(modalities)\n",
    "    \n",
    "    return activity_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "activity_report = generate_activity_report(directory)\n",
    "\n",
    "# Display the activity report\n",
    "print(\"DICOM Activity Report:\")\n",
    "for key, value in activity_report.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHI extract dicom attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dicom_attributes(directory, output_path, location='local'):\n",
    "    \"\"\"\n",
    "    Recursively extract specific attributes from DICOM files in the input directory and save to an Excel file.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The directory containing DICOM files (local directory path or S3 bucket prefix).\n",
    "    - output_path (str): The path to save the Excel file.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    unique_values = set()\n",
    "    data = []\n",
    "    \n",
    "    if location == 'local':\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            # Keep track of series UID and the number of files in each series\n",
    "            series_count = {}\n",
    "\n",
    "            for file in files:\n",
    "                if file.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    ds = pydicom.dcmread(file_path)\n",
    "\n",
    "                    series_uid = ds.SeriesInstanceUID\n",
    "\n",
    "                    # Count the number of files in the series\n",
    "                    series_count[series_uid] = series_count.get(series_uid, 0) + 1\n",
    "\n",
    "                    for elem in ds:\n",
    "                        value = str(ds[elem.tag].value)\n",
    "                        if value not in unique_values:\n",
    "                            unique_values.add(value)\n",
    "\n",
    "                            # Extract desired attributes\n",
    "                            element = elem.name\n",
    "                            vr = elem.VR\n",
    "                            q_value = elem.VR\n",
    "                            description = elem.description\n",
    "                            disp = elem.description\n",
    "                            num_series = series_count.get(series_uid, 0)\n",
    "\n",
    "                            data.append({'File': file_path, 'Element': element, 'VR': vr, 'Q-value': q_value, \n",
    "                                         'Description': description, 'Disp': disp, 'Num_series': num_series})\n",
    "                            break  # Move to the next DICOM file after extracting one unique tag value\n",
    "    elif location == 's3':\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    # Read DICOM file directly from S3\n",
    "                    try:\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "\n",
    "                        series_uid = ds.SeriesInstanceUID\n",
    "\n",
    "                        # Count the number of files in the series\n",
    "                        series_count[series_uid] = series_count.get(series_uid, 0) + 1\n",
    "\n",
    "                        for elem in ds:\n",
    "                            value = str(ds[elem.tag].value)\n",
    "                            if value not in unique_values:\n",
    "                                unique_values.add(value)\n",
    "\n",
    "                                # Extract desired attributes\n",
    "                                element = elem.name\n",
    "                                vr = elem.VR\n",
    "                                q_value = elem.VR\n",
    "                                description = elem.description\n",
    "                                disp = elem.description\n",
    "                                num_series = series_count.get(series_uid, 0)\n",
    "\n",
    "                                data.append({'File': key, 'Element': element, 'VR': vr, 'Q-value': q_value, \n",
    "                                             'Description': description, 'Disp': disp, 'Num_series': num_series})\n",
    "                                break  # Move to the next DICOM file after extracting one unique tag value\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading DICOM file '{key}' from S3: {e}\")\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "directory, location = get_directory_path()\n",
    "output_path = r'your path'\n",
    "extract_dicom_attributes(directory, output_path, 's3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_phi(directory, substrings, location='local'):\n",
    "    \"\"\"\n",
    "    Remove DICOM tags containing substrings entered by the user.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files (local directory path or S3 bucket prefix).\n",
    "    - substrings (list): List of substrings to search for in DICOM tags.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if location == 'local':\n",
    "        dicom_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.dcm')]\n",
    "        dicom_files.sort()\n",
    "\n",
    "        for file_path in dicom_files:\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "\n",
    "            # Remove tags containing specified substrings\n",
    "            for tag in ds:\n",
    "                for substring in substrings:\n",
    "                    if substring.lower() in tag.lower():\n",
    "                        delattr(ds, tag)\n",
    "                        break\n",
    "\n",
    "            # Save modified DICOM\n",
    "            output_path = os.path.splitext(file_path)[0] + '_no_phi.dcm'\n",
    "            ds.save_as(output_path)\n",
    "\n",
    "            print(f\"Removed specified substrings from {file_path} and saved as {output_path}\")\n",
    "    elif location == 's3':\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        # Read DICOM file directly from S3\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "\n",
    "                        # Remove tags containing specified substrings\n",
    "                        for tag in ds:\n",
    "                            for substring in substrings:\n",
    "                                if substring.lower() in tag.lower():\n",
    "                                    delattr(ds, tag)\n",
    "                                    break\n",
    "\n",
    "                        # Save modified DICOM\n",
    "                        output_key = os.path.splitext(key)[0] + '_no_phi.dcm'\n",
    "                        with open(output_key, 'wb') as f:\n",
    "                            ds.save_as(f)\n",
    "\n",
    "                        print(f\"Removed specified substrings from {key} and saved as {output_key}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing DICOM file '{key}' from S3: {e}\")\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(directory, output_path, location='local'):\n",
    "    \"\"\"\n",
    "    Generate a spreadsheet summary of DICOM tags for review.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): The root directory containing DICOM files (local directory path or S3 bucket prefix).\n",
    "    - output_path (str): The path to save the Excel file.\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if location == 'local':\n",
    "        dicom_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.dcm')]\n",
    "        dicom_files.sort()\n",
    "\n",
    "        dicom_data = []\n",
    "\n",
    "        for file_path in dicom_files:\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            dicom_info = {'File': file_path}\n",
    "\n",
    "            for elem in ds:\n",
    "                dicom_info[elem.name] = str(elem.value)\n",
    "\n",
    "            dicom_data.append(dicom_info)\n",
    "\n",
    "        df = pd.DataFrame(dicom_data)\n",
    "        df.to_excel(output_path, index=False)\n",
    "        print(f\"Summary of DICOM tags saved to {output_path}\")\n",
    "    elif location == 's3':\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "        if 'Contents' in response:\n",
    "            dicom_data = []\n",
    "\n",
    "            for obj in response['Contents']:\n",
    "                key = obj['Key']\n",
    "                if key.endswith('.dcm'):\n",
    "                    try:\n",
    "                        # Read DICOM file directly from S3\n",
    "                        s3_object = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "                        ds = pydicom.dcmread(s3_object['Body'])\n",
    "\n",
    "                        dicom_info = {'File': key}\n",
    "\n",
    "                        for elem in ds:\n",
    "                            dicom_info[elem.name] = str(elem.value)\n",
    "\n",
    "                        dicom_data.append(dicom_info)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing DICOM file '{key}' from S3: {e}\")\n",
    "\n",
    "            df = pd.DataFrame(dicom_data)\n",
    "            df.to_excel(output_path, index=False)\n",
    "            print(f\"Summary of DICOM tags saved to {output_path}\")\n",
    "        else:\n",
    "            print(\"No DICOM files found in the specified S3 bucket location.\")\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the directory containing DICOM files\n",
    "#directory = '/path/to/your/dicom/directory/'\n",
    "\n",
    "directory, location = get_directory_path()\n",
    "\n",
    "# Enter substrings to remove from DICOM tags\n",
    "substrings = input(\"Enter substrings to remove from DICOM tags (comma-separated): \").split(',')\n",
    "\n",
    "# Remove specified substrings and save modified DICOM files\n",
    "remove_phi(directory, substrings)\n",
    "\n",
    "# Generate a spreadsheet summary of DICOM tags for review\n",
    "output_path = r'your path'\n",
    "generate_summary(directory, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename Files & Check Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_dicom_files(input_directory, location='local'):\n",
    "    \"\"\"\n",
    "    Rename DICOM files according to their SOP Instance UID and empty Age tag if age > 89.\n",
    "\n",
    "    Args:\n",
    "    - input_directory (str): The input directory containing DICOM files.\n",
    "    - location (str): Location type, either 'local' or 's3'. Default is 'local'.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if location == 'local':\n",
    "        dcm_files = glob.glob(os.path.join(input_directory, \"**/*.dcm\"), recursive=True)\n",
    "    elif location == 's3':\n",
    "        s3 = boto3.resource('s3')\n",
    "        bucket_name, prefix = input_directory.split('/', 1)\n",
    "        bucket = s3.Bucket(bucket_name)\n",
    "        dcm_files = [obj.key for obj in bucket.objects.filter(Prefix=prefix) if obj.key.endswith('.dcm')]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid location type. Use 'local' or 's3'.\")\n",
    "\n",
    "    print(f\"{len(dcm_files)} DICOM files found in the directory.\\n\")\n",
    "\n",
    "    for dcm_file in tqdm(dcm_files):\n",
    "        if location == 'local':\n",
    "            complete_f_path = dcm_file\n",
    "        elif location == 's3':\n",
    "            file_name = os.path.basename(dcm_file)\n",
    "            download_path = os.path.join('/tmp', file_name)\n",
    "            try:\n",
    "                bucket.download_file(dcm_file, download_path)\n",
    "            except botocore.exceptions.ClientError as e:\n",
    "                if e.response['Error']['Code'] == \"404\":\n",
    "                    print(f\"The object {dcm_file} does not exist.\")\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "            complete_f_path = download_path\n",
    "\n",
    "        ds = pydicom.dcmread(complete_f_path)\n",
    "        sop_instance_uid = str(ds.get((0x0008, 0x0018)).value)\n",
    "\n",
    "        if (0x0010, 0x1010) in ds:\n",
    "            age = str(ds.PatientAge)\n",
    "            if age:\n",
    "                scale = age[-1:]\n",
    "                if scale.isalpha():\n",
    "                    age = age[:-1]\n",
    "                if len(age) > 1:\n",
    "                    if age[0] == '0':\n",
    "                        age = int(age[1:])\n",
    "                    else:\n",
    "                        age = int(age)\n",
    "                elif len(age) <= 1:\n",
    "                    age = int(age)\n",
    "                if age > 89:\n",
    "                    ds.PatientAge = None\n",
    "\n",
    "        folder = os.path.dirname(complete_f_path)\n",
    "        destination = os.path.join(folder, f\"{sop_instance_uid}.dcm\")\n",
    "\n",
    "        os.rename(complete_f_path, destination)\n",
    "        ds.save_as(destination)\n",
    "\n",
    "    print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# For local directory:\n",
    "rename_dicom_files(\"/path/to/local/directory\", location='local')\n",
    "\n",
    "# For S3 directory:\n",
    "# rename_dicom_files(\"your_bucket_name/your_prefix\", location='s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate DME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_metadata(file):\n",
    "    \"\"\"\n",
    "    Extracts DICOM metadata from a DICOM file.\n",
    "    \n",
    "    Args:\n",
    "    - file (str): Path to the DICOM file.\n",
    "    \n",
    "    Returns:\n",
    "    - data (dict): Dictionary containing DICOM metadata.\n",
    "    \"\"\"\n",
    "    dcm = pydicom.dcmread(file)\n",
    "    pixel_spacing = dcm.get(\"PixelSpacing\", \"\")\n",
    "    if pixel_spacing != \"\":\n",
    "        pixel_spacing = pixel_spacing[0].original_string\n",
    "    imager_pixel_spacing = dcm.get(\"ImagerPixelSpacing\", \"\")\n",
    "    if imager_pixel_spacing != \"\":\n",
    "        imager_pixel_spacing = imager_pixel_spacing[0].original_string\n",
    "    image_type = dcm.get(\"ImageType\", \"\")\n",
    "    if type(image_type) != str:\n",
    "        image_type = \"_\".join(dcm.get(\"ImageType\", \"\"))\n",
    "    convolution_kernel = dcm.get(\"ConvolutionKernel\", \"\")\n",
    "    if type(convolution_kernel) != str:\n",
    "        convolution_kernel = \"_\".join(dcm.get(\"ConvolutionKernel\", \"\"))\n",
    "    exposure_modulation_type = dcm.get(\"ExposureModulationType\", \"\")\n",
    "    if type(exposure_modulation_type) != str:\n",
    "        exposure_modulation_type = \"_\".join(dcm.get(\"ExposureModulationType\", \"\"))\n",
    "\n",
    "    data = {\n",
    "        'file_name': os.path.basename(file),\n",
    "        'accession_number': dcm.get(\"AccessionNumber\", \"\"),\n",
    "        'acquisition_type': dcm.get(\"AcquisitionType\", \"\"),\n",
    "        'body_part_examined': dcm.get(\"BodyPartExamined\", \"\"),\n",
    "        'case_ids': dcm.get(\"PatientID\", \"\"),\n",
    "        'contrast_bolus_agent': dcm.get(\"ContrastBolusAgent\", \"\"),\n",
    "        'patient_position': dcm.get(\"PatientPosition\", \"\"),\n",
    "        'convolution_kernel': convolution_kernel,\n",
    "        'detector_type': dcm.get(\"DetectorType\", \"\"),\n",
    "        'exposure_modulation_type': exposure_modulation_type,\n",
    "        'image_type': image_type,\n",
    "        'imager_pixel_spacing': imager_pixel_spacing,\n",
    "        'lossy_image_compression': dcm.get(\"LossyImageCompression\", \"\"),\n",
    "        'manufacturer': dcm.get(\"Manufacturer\", \"\"),\n",
    "        'manufacturer_model_name': dcm.get(\"ManufacturerModelName\", \"\"),\n",
    "        'modality': dcm.get(\"Modality\", \"\"),\n",
    "        'sop_instance_uid': dcm.get(\"SOPInstanceUID\", \"\"),\n",
    "        'pixel_spacing': pixel_spacing,\n",
    "        'series_description': dcm.get(\"SeriesDescription\", \"\"),\n",
    "        'series_uid': dcm.get(\"SeriesInstanceUID\", \"\"),\n",
    "        'slice_thickness': dcm.get(\"SliceThickness\", \"\"),\n",
    "        'spacing_between_slices': dcm.get(\"SpacingBetweenSlices\", \"\"),\n",
    "        'spatial_resolution': dcm.get(\"SpatialResolution\", \"\"),\n",
    "        'study_description': dcm.get(\"StudyDescription\", \"\"),\n",
    "        'study_uid': dcm.get(\"StudyInstanceUID\", \"\"),\n",
    "        'view_position': dcm.get(\"ViewPosition\", \"\"),\n",
    "        'study_date': dcm.get(\"StudyDate\", \"\")\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Writes DICOM metadata to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - data (list): List of dictionaries containing DICOM metadata.\n",
    "    - filename (str): Path to the output CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def loc_dicom_files(directory):\n",
    "    \"\"\"\n",
    "    Locates DICOM files in a local directory.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): Path to the local directory.\n",
    "    \n",
    "    Returns:\n",
    "    - dicom_files (list): List of paths to DICOM files.\n",
    "    \"\"\"\n",
    "    dicom_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".dcm\"):\n",
    "                dicom_files.append(os.path.join(root, file))\n",
    "    return dicom_files\n",
    "\n",
    "\n",
    "def s3_dicom_files(bucket_name, prefix):\n",
    "    \"\"\"\n",
    "    Locates DICOM files in an S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "    - bucket_name (str): Name of the S3 bucket.\n",
    "    - prefix (str): Prefix to search within the bucket.\n",
    "    \n",
    "    Returns:\n",
    "    - dicom_files (list): List of paths to DICOM files.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    dicom_files = []\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.endswith('.dcm'):\n",
    "                dicom_files.append(key)\n",
    "\n",
    "    return dicom_files\n",
    "\n",
    "\n",
    "def DME_main(directory, location='local'):\n",
    "    \"\"\"\n",
    "    Generates DICOM metadata CSV file.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): Path to the directory (local directory path or S3 bucket prefix).\n",
    "    - location (str): The location type ('local' or 's3'). Default is 'local'.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if location == 'local':\n",
    "        dicom_files = loc_dicom_files(directory)\n",
    "    elif location == 's3':\n",
    "        bucket_name, prefix = directory.split('/', 1)\n",
    "        dicom_files = s3_dicom_files(bucket_name, prefix)\n",
    "    else:\n",
    "        print(\"Invalid location type. Please provide 'local' or 's3'.\")\n",
    "        return\n",
    "\n",
    "    data = [get_dicom_metadata(file) for file in tqdm(dicom_files, desc='Progress')]\n",
    "    output_file = os.path.basename(os.path.normpath(directory)) + \"_dcm_metadata\" + \".csv\"\n",
    "    write_csv(data, output_file)\n",
    "    print()\n",
    "    print('DICOM metadata exported to ' + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Usage\n",
    "directory = f'{bucket_name}/{prefix}'\n",
    "DME_mainmain(directory, location='s3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
